{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "1c98bae3b34254e00ad38852223e0db4369a8f45fc93c0ffced3852f66cb25c7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Goal\n",
    "\n",
    "We are going to train a ML model for predicting the rating of a chocolate bar given the data in our falvors_of_cacao.csv file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Let's Take a Look At Our Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('flavors_of_cacao.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  Company Maker Specific Bean Origin\\nOr Bar Name   REF  Review\\nDate  \\\n",
       "0      A. Morin                       Agua Grande  1876          2016   \n",
       "1      A. Morin                             Kpime  1676          2015   \n",
       "2      A. Morin                            Atsane  1676          2015   \n",
       "3      A. Morin                             Akata  1680          2015   \n",
       "4      A. Morin                            Quilla  1704          2015   \n",
       "\n",
       "  Cocoa\\nPercent Company\\nLocation  Rating Bean\\nType Broad Bean\\nOrigin  \n",
       "0            63%            France    3.75                      Sao Tome  \n",
       "1            70%            France    2.75                          Togo  \n",
       "2            70%            France    3.00                          Togo  \n",
       "3            70%            France    3.50                          Togo  \n",
       "4            70%            France    3.50                          Peru  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Company Maker</th>\n      <th>Specific Bean Origin\\nOr Bar Name</th>\n      <th>REF</th>\n      <th>Review\\nDate</th>\n      <th>Cocoa\\nPercent</th>\n      <th>Company\\nLocation</th>\n      <th>Rating</th>\n      <th>Bean\\nType</th>\n      <th>Broad Bean\\nOrigin</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A. Morin</td>\n      <td>Agua Grande</td>\n      <td>1876</td>\n      <td>2016</td>\n      <td>63%</td>\n      <td>France</td>\n      <td>3.75</td>\n      <td></td>\n      <td>Sao Tome</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A. Morin</td>\n      <td>Kpime</td>\n      <td>1676</td>\n      <td>2015</td>\n      <td>70%</td>\n      <td>France</td>\n      <td>2.75</td>\n      <td></td>\n      <td>Togo</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A. Morin</td>\n      <td>Atsane</td>\n      <td>1676</td>\n      <td>2015</td>\n      <td>70%</td>\n      <td>France</td>\n      <td>3.00</td>\n      <td></td>\n      <td>Togo</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A. Morin</td>\n      <td>Akata</td>\n      <td>1680</td>\n      <td>2015</td>\n      <td>70%</td>\n      <td>France</td>\n      <td>3.50</td>\n      <td></td>\n      <td>Togo</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A. Morin</td>\n      <td>Quilla</td>\n      <td>1704</td>\n      <td>2015</td>\n      <td>70%</td>\n      <td>France</td>\n      <td>3.50</td>\n      <td></td>\n      <td>Peru</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Company Maker\n\nSpecific Bean Origin\nOr Bar Name\n\nREF\n\nReview\nDate\n\nCocoa\nPercent\n\nCompany\nLocation\n\nRating\n\nBean\nType\n\nBroad Bean\nOrigin\n\n"
     ]
    }
   ],
   "source": [
    "for column in data.columns:\n",
    "    print(column + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of records total in csv file: 1795\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of records total in csv file: {len(data)}')"
   ]
  },
  {
   "source": [
    "### Target Vector\n",
    "\n",
    "Looking at the data, we will use the \"Rating\" column to be our target vector.\n",
    "\n",
    "However, before we do anything, we must clean our data and then transform it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data Cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Column Name\n",
    "\n",
    "First thing is first, notice that the names of most of our columns contain a \"\\n\" in them. We are going to need to change this, as it will later become a parsing issue.\n",
    "\n",
    "Generally we want to avoid any form of white space in the name of our columns.\n",
    "\n",
    "E.g. ThisIsAGoodFormatForColumnNames\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['CompanyMaker', 'SpecificBeanOriginOrBarName', 'REF', 'ReviewDate',\n       'CocoaPercent', 'CompanyLocation', 'Rating', 'BeanType',\n       'BroadBeanOrigin'],\n      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data.columns = data.columns.str.replace('\\\\n', '')\n",
    "data.columns = data.columns.str.replace(' ', '')\n",
    "\n",
    "print(data.columns)"
   ]
  },
  {
   "source": [
    "### Missing Values\n",
    "\n",
    "First thing's first, let's take a look at each column of data that we have and get the number of missing values for that  column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CompanyMaker                   0\nSpecificBeanOriginOrBarName    0\nREF                            0\nReviewDate                     0\nCocoaPercent                   0\nCompanyLocation                0\nRating                         0\nBeanType                       1\nBroadBeanOrigin                1\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "source": [
    "It appears that barely any of the data is missing. I mean at the maximum amount, there is 1 record that is Nan, so we will just go ahead and remove every row that contains a nan."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of NA values: 2\nNumber of values before removing NA: 1795\nNumber of values after removing NA: 1793\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'Total number of NA values: {data.isnull().sum().sum()}')\n",
    "print(f'Number of values before removing NA: {len(data)}')\n",
    "data = data.dropna(axis=0)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(f'Number of values after removing NA: {len(data)}')"
   ]
  },
  {
   "source": [
    "### Let's take a look at each Columns min and max values (especially the integer fields)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "326     1.0\n437     1.0\n465     1.0\n1174    1.0\n245     1.5\n249     1.5\n324     1.5\n449     1.5\n554     1.5\n988     1.5\nName: Rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(data.nsmallest(10, 'Rating')['Rating'])"
   ]
  },
  {
   "source": [
    "This also looks good. If any reviews were less than 1, I would have removed them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Overall data set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Smallest values:\n\nCompanyMaker                                      A. Morin\nSpecificBeanOriginOrBarName    \"heirloom\", Arriba Nacional\nREF                                                      5\nReviewDate                                            2006\nCocoaPercent                                          100%\nCompanyLocation                                  Amsterdam\nRating                                                   1\nBeanType                                            Amazon\nBroadBeanOrigin                  Africa, Carribean, C. Am.\ndtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Smallest values:\\n\")\n",
    "print(data.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Largest values:\n\nCompanyMaker                                     twenty-four blackbirds\nSpecificBeanOriginOrBarName    the lost city, gracias a dias, batch 362\nREF                                                                1952\nReviewDate                                                         2017\nCocoaPercent                                                        99%\nCompanyLocation                                                   Wales\nRating                                                                5\nBeanType                                                               \nBroadBeanOrigin                                                        \ndtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Largest values:\\n\")\n",
    "print(data.max())"
   ]
  },
  {
   "source": [
    "### Remove REF\n",
    "\n",
    "The REF field will not give us any information about the likeability of a chocolate bar, so we will remove it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "data = data.drop(['REF'], axis=1)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['CompanyMaker', 'SpecificBeanOriginOrBarName', 'ReviewDate',\n       'CocoaPercent', 'CompanyLocation', 'Rating', 'BeanType',\n       'BroadBeanOrigin'],\n      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "source": [
    "## Data Transformation\n",
    "\n",
    "We need to take ever column of string data and convert each entry into integer form.\n",
    "\n",
    "This is required for running any machine learning algorithms on our data.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  CompanyMaker SpecificBeanOriginOrBarName  ReviewDate CocoaPercent  \\\n0     A. Morin                 Agua Grande        2016          63%   \n\n  CompanyLocation  Rating BeanType BroadBeanOrigin  \n0          France    3.75                 Sao Tome  \n"
     ]
    }
   ],
   "source": [
    "print(data.head(1))"
   ]
  },
  {
   "source": [
    "The fields that need to be converted to integer are everything except for \"Rating\", and \"Review Date\".\n",
    "\n",
    "We will also have to deal with CocoaPercent, but we will deal with this one differently."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Let's deal with converting Cocoa Percent into a integer column\n",
    "\n",
    "First we will convert this field and store it off as its own data frame.\n",
    "\n",
    "Later we will combine this together with another dataframe to put everything back together but in integer form.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   CocoaPercentage\n0             63.0\n1             70.0\n2             70.0\n3             70.0\n4             70.0\n"
     ]
    }
   ],
   "source": [
    "# This data is almost in correct integer form as it is.\n",
    "# All we need to do it cut off the % sign from each entry and convert that str to type float.\n",
    "cocoa_percent_dict = {'CocoaPercentage': [float(value.replace('%', '')) for value in data['CocoaPercent']]}\n",
    "\n",
    "# And boom, now we have a valid data frame.\n",
    "cocoa_percent_df = pd.DataFrame(cocoa_percent_dict)\n",
    "print(cocoa_percent_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Min: CocoaPercentage    42.0\ndtype: float64\nMax: CocoaPercentage    100.0\ndtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Just for curiosity, I want to look at the max and min.\n",
    "print(f\"Min: {cocoa_percent_df.min()}\")\n",
    "print(f\"Max: {cocoa_percent_df.max()}\")"
   ]
  },
  {
   "source": [
    "#### Now it's time to deal with the all of the other string fields.\n",
    "\n",
    "Note: Unlike the \"Cocoa Percent\" field,  this these fields don't have a level of ordinal valuing. As in there is no order to this data, high values do not mean that the data is better or worse, it is simply a label. These fields are simply nominal.\n",
    "\n",
    "Since we are dealing with nomial data (and NOT ordinal), we are going to run One-Hot-Encoding on all nominal fields.\n",
    "\n",
    "Pro: Each category of data will not be taintedby calculations where one field has a large value then the other (like ordinal data).\n",
    "\n",
    "Con: Because of the way One-Hot-Encoding works, we will have an additional vector (column) for each version of the encoded data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reminder of what columns we have:\n\nCompanyMaker\nSpecificBeanOriginOrBarName\nReviewDate\nCocoaPercent\nCompanyLocation\nRating\nBeanType\nBroadBeanOrigin\n"
     ]
    }
   ],
   "source": [
    "print(\"Reminder of what columns we have:\\n\")\n",
    "for col in data.columns:\n",
    "    print(col)"
   ]
  },
  {
   "source": [
    "### We will create a new DataFrame that will hold everything in integer form\n",
    "\n",
    "This will be the new dataframe that we are going to insert all of our integer columns into, now that everything is going to be converted into integer form.\n",
    "\n",
    "We will take the columns from data that are already in the correct form. Then we will add the fields that are going to be properly encoded in later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['Rating', 'ReviewDate', 'CocoaPercentage'], dtype='object')\n   Rating  ReviewDate  CocoaPercentage\n0    3.75        2016             63.0\n1    2.75        2015             70.0\n2    3.00        2015             70.0\n3    3.50        2015             70.0\n4    3.50        2015             70.0\n"
     ]
    }
   ],
   "source": [
    "new_data = data[['Rating', 'ReviewDate']]\n",
    "\n",
    "# Let's add in the data frame that we had created for converting CocoaPercent into integer form.\n",
    "new_data = pd.concat([new_data, cocoa_percent_df], axis=1)\n",
    "\n",
    "print(new_data.columns)\n",
    "print(new_data.head())\n"
   ]
  },
  {
   "source": [
    "### Created One-Hot-Encoded DataFrame\n",
    "\n",
    "We will take all of the nomial fields now and run one hot encoding on them to get them into integer form."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "print(f'Nomial fields: {data.columns}\\n')\n",
    "\n",
    "# Are there any featires tjat we wannt to test out dropping\n",
    "# list_to_drop = ['BroadBeanOrigin', 'CompanyMaker', 'CompanyLocation', 'SpecificBeanOriginOrBarName']\n",
    "# data = data.drop(list_to_drop, axis=1)\n",
    "\n",
    "nomial_fields = [col for col in data.columns if col not in ['Rating', 'ReviewDate', 'CocoaPercent']]\n",
    "nomial_fields_df = data[nomial_fields]\n",
    "one_hot_encoded_df = pd.get_dummies(nomial_fields_df, drop_first=True)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nomial fields: Index(['CompanyMaker', 'SpecificBeanOriginOrBarName', 'ReviewDate',\n       'CocoaPercent', 'CompanyLocation', 'Rating', 'BeanType',\n       'BroadBeanOrigin'],\n      dtype='object')\n\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Expected shape of OHE matrix: (1793, 8)\nActual shape of OHE matrix: (1793, 1649)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Expected shape of OHE matrix: {data.shape}\")\n",
    "print(f\"Actual shape of OHE matrix: {one_hot_encoded_df.shape}\")"
   ]
  },
  {
   "source": [
    "Let's finish the construction of the \"new_data\" DataFrame that we were creating. Recall that this DataFrame will contain all of our one hot encoded fields and the other interger fields that we already have."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Rating  ReviewDate  CocoaPercentage  CompanyMaker_AMMA  \\\n",
      "0    3.75        2016             63.0                  0   \n",
      "1    2.75        2015             70.0                  0   \n",
      "2    3.00        2015             70.0                  0   \n",
      "3    3.50        2015             70.0                  0   \n",
      "4    3.50        2015             70.0                  0   \n",
      "\n",
      "   CompanyMaker_Acalli  CompanyMaker_Adi  CompanyMaker_Aequare (Gianduja)  \\\n",
      "0                    0                 0                                0   \n",
      "1                    0                 0                                0   \n",
      "2                    0                 0                                0   \n",
      "3                    0                 0                                0   \n",
      "4                    0                 0                                0   \n",
      "\n",
      "   CompanyMaker_Ah Cacao  CompanyMaker_Akesson's (Pralus)  \\\n",
      "0                      0                                0   \n",
      "1                      0                                0   \n",
      "2                      0                                0   \n",
      "3                      0                                0   \n",
      "4                      0                                0   \n",
      "\n",
      "   CompanyMaker_Alain Ducasse  ...  BroadBeanOrigin_Venezuela  \\\n",
      "0                           0  ...                          0   \n",
      "1                           0  ...                          0   \n",
      "2                           0  ...                          0   \n",
      "3                           0  ...                          0   \n",
      "4                           0  ...                          0   \n",
      "\n",
      "   BroadBeanOrigin_Venezuela, Carribean  BroadBeanOrigin_Venezuela, Dom. Rep.  \\\n",
      "0                                     0                                     0   \n",
      "1                                     0                                     0   \n",
      "2                                     0                                     0   \n",
      "3                                     0                                     0   \n",
      "4                                     0                                     0   \n",
      "\n",
      "   BroadBeanOrigin_Venezuela, Ghana  BroadBeanOrigin_Venezuela, Java  \\\n",
      "0                                 0                                0   \n",
      "1                                 0                                0   \n",
      "2                                 0                                0   \n",
      "3                                 0                                0   \n",
      "4                                 0                                0   \n",
      "\n",
      "   BroadBeanOrigin_Venezuela, Trinidad  BroadBeanOrigin_Venezuela/ Ghana  \\\n",
      "0                                    0                                 0   \n",
      "1                                    0                                 0   \n",
      "2                                    0                                 0   \n",
      "3                                    0                                 0   \n",
      "4                                    0                                 0   \n",
      "\n",
      "   BroadBeanOrigin_Vietnam  BroadBeanOrigin_West Africa  BroadBeanOrigin_   \n",
      "0                        0                            0                  0  \n",
      "1                        0                            0                  0  \n",
      "2                        0                            0                  0  \n",
      "3                        0                            0                  0  \n",
      "4                        0                            0                  0  \n",
      "\n",
      "[5 rows x 1652 columns]\n"
     ]
    }
   ],
   "source": [
    "new_data = pd.concat([new_data, one_hot_encoded_df], axis=1)\n",
    "print(new_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1793, 1652)\n"
     ]
    }
   ],
   "source": [
    "print(new_data.shape)"
   ]
  },
  {
   "source": [
    "## Feature Reduction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Taking a look at the number of columns that we have, it is really easy to see that we have...a LOT more features then we started with.\n",
    "\n",
    "In fact, after running One Hot Encoding, we now have 1649 different columns that are just associated with the nomial fields that we had.\n",
    "\n",
    "This will definately impact the speed it takes to create our model, among other things.\n",
    "\n",
    "To combat this we will run feature reduction using the PCA algorthim. We will determine what features are useful and what features are not."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Standardize our data\n",
    "\n",
    "Before we can run PCA on our data, we need to standardize it. All of our X data should be within 0 - 1 of itself. This will help to avoid (issues with gradient descent later) and is required for PCA. Data must be in the correct range, or the varience will not make any sense."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "finalized_feature_matrix = pd.DataFrame(scaler.fit_transform(new_data))\n",
    "print(finalized_feature_matrix)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "          0         1         2         3         4         5         6     \\\n0     1.181356  1.254754 -1.375407 -0.052881 -0.033417 -0.047285 -0.033417   \n1    -0.912734  0.913207 -0.268644 -0.052881 -0.033417 -0.047285 -0.033417   \n2    -0.389211  0.913207 -0.268644 -0.052881 -0.033417 -0.047285 -0.033417   \n3     0.657834  0.913207 -0.268644 -0.052881 -0.033417 -0.047285 -0.033417   \n4     0.657834  0.913207 -0.268644 -0.052881 -0.033417 -0.047285 -0.033417   \n...        ...       ...       ...       ...       ...       ...       ...   \n1788  1.181356 -0.452984 -0.268644 -0.052881 -0.033417 -0.047285 -0.033417   \n1789 -0.389211 -0.452984 -1.059189 -0.052881 -0.033417 -0.047285 -0.033417   \n1790  0.657834 -0.452984 -1.059189 -0.052881 -0.033417 -0.047285 -0.033417   \n1791  0.134311 -0.452984 -1.533516 -0.052881 -0.033417 -0.047285 -0.033417   \n1792 -0.389211 -0.794532 -1.059189 -0.052881 -0.033417 -0.047285 -0.033417   \n\n          7         8         9     ...      1642      1643      1644  \\\n0    -0.023623 -0.040939 -0.052881  ... -0.368142 -0.023623 -0.023623   \n1    -0.023623 -0.040939 -0.052881  ... -0.368142 -0.023623 -0.023623   \n2    -0.023623 -0.040939 -0.052881  ... -0.368142 -0.023623 -0.023623   \n3    -0.023623 -0.040939 -0.052881  ... -0.368142 -0.023623 -0.023623   \n4    -0.023623 -0.040939 -0.052881  ... -0.368142 -0.023623 -0.023623   \n...        ...       ...       ...  ...       ...       ...       ...   \n1788 -0.023623 -0.040939 -0.052881  ... -0.368142 -0.023623 -0.023623   \n1789 -0.023623 -0.040939 -0.052881  ... -0.368142 -0.023623 -0.023623   \n1790 -0.023623 -0.040939 -0.052881  ... -0.368142 -0.023623 -0.023623   \n1791 -0.023623 -0.040939 -0.052881  ... -0.368142 -0.023623 -0.023623   \n1792 -0.023623 -0.040939 -0.052881  ... -0.368142 -0.023623 -0.023623   \n\n          1645      1646      1647      1648      1649      1650      1651  \n0    -0.023623 -0.023623 -0.023623 -0.023623 -0.147148 -0.057945 -0.206014  \n1    -0.023623 -0.023623 -0.023623 -0.023623 -0.147148 -0.057945 -0.206014  \n2    -0.023623 -0.023623 -0.023623 -0.023623 -0.147148 -0.057945 -0.206014  \n3    -0.023623 -0.023623 -0.023623 -0.023623 -0.147148 -0.057945 -0.206014  \n4    -0.023623 -0.023623 -0.023623 -0.023623 -0.147148 -0.057945 -0.206014  \n...        ...       ...       ...       ...       ...       ...       ...  \n1788 -0.023623 -0.023623 -0.023623 -0.023623 -0.147148 -0.057945 -0.206014  \n1789 -0.023623 -0.023623 -0.023623 -0.023623 -0.147148 -0.057945 -0.206014  \n1790 -0.023623 -0.023623 -0.023623 -0.023623 -0.147148 -0.057945 -0.206014  \n1791 -0.023623 -0.023623 -0.023623 -0.023623 -0.147148 -0.057945 -0.206014  \n1792 -0.023623 -0.023623 -0.023623 -0.023623 -0.147148 -0.057945 -0.206014  \n\n[1793 rows x 1652 columns]\n"
     ]
    }
   ]
  },
  {
   "source": [
    "By stanardizing our data using Sklearn, we have lost our data column names.\n",
    "\n",
    "We are going to need to add those back so we know what feature is what and such."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Since we are stanardazing our data, the mean should be close to 0.0 and the standard deviation should be about 1.0"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "        Rating  ReviewDate  CocoaPercentage  CompanyMaker_AMMA  \\\n0     1.181356    1.254754        -1.375407          -0.052881   \n1    -0.912734    0.913207        -0.268644          -0.052881   \n2    -0.389211    0.913207        -0.268644          -0.052881   \n3     0.657834    0.913207        -0.268644          -0.052881   \n4     0.657834    0.913207        -0.268644          -0.052881   \n...        ...         ...              ...                ...   \n1788  1.181356   -0.452984        -0.268644          -0.052881   \n1789 -0.389211   -0.452984        -1.059189          -0.052881   \n1790  0.657834   -0.452984        -1.059189          -0.052881   \n1791  0.134311   -0.452984        -1.533516          -0.052881   \n1792 -0.389211   -0.794532        -1.059189          -0.052881   \n\n      CompanyMaker_Acalli  CompanyMaker_Adi  CompanyMaker_Aequare (Gianduja)  \\\n0               -0.033417         -0.047285                        -0.033417   \n1               -0.033417         -0.047285                        -0.033417   \n2               -0.033417         -0.047285                        -0.033417   \n3               -0.033417         -0.047285                        -0.033417   \n4               -0.033417         -0.047285                        -0.033417   \n...                   ...               ...                              ...   \n1788            -0.033417         -0.047285                        -0.033417   \n1789            -0.033417         -0.047285                        -0.033417   \n1790            -0.033417         -0.047285                        -0.033417   \n1791            -0.033417         -0.047285                        -0.033417   \n1792            -0.033417         -0.047285                        -0.033417   \n\n      CompanyMaker_Ah Cacao  CompanyMaker_Akesson's (Pralus)  \\\n0                 -0.023623                        -0.040939   \n1                 -0.023623                        -0.040939   \n2                 -0.023623                        -0.040939   \n3                 -0.023623                        -0.040939   \n4                 -0.023623                        -0.040939   \n...                     ...                              ...   \n1788              -0.023623                        -0.040939   \n1789              -0.023623                        -0.040939   \n1790              -0.023623                        -0.040939   \n1791              -0.023623                        -0.040939   \n1792              -0.023623                        -0.040939   \n\n      CompanyMaker_Alain Ducasse  ...  BroadBeanOrigin_Venezuela  \\\n0                      -0.052881  ...                  -0.368142   \n1                      -0.052881  ...                  -0.368142   \n2                      -0.052881  ...                  -0.368142   \n3                      -0.052881  ...                  -0.368142   \n4                      -0.052881  ...                  -0.368142   \n...                          ...  ...                        ...   \n1788                   -0.052881  ...                  -0.368142   \n1789                   -0.052881  ...                  -0.368142   \n1790                   -0.052881  ...                  -0.368142   \n1791                   -0.052881  ...                  -0.368142   \n1792                   -0.052881  ...                  -0.368142   \n\n      BroadBeanOrigin_Venezuela, Carribean  \\\n0                                -0.023623   \n1                                -0.023623   \n2                                -0.023623   \n3                                -0.023623   \n4                                -0.023623   \n...                                    ...   \n1788                             -0.023623   \n1789                             -0.023623   \n1790                             -0.023623   \n1791                             -0.023623   \n1792                             -0.023623   \n\n      BroadBeanOrigin_Venezuela, Dom. Rep.  BroadBeanOrigin_Venezuela, Ghana  \\\n0                                -0.023623                         -0.023623   \n1                                -0.023623                         -0.023623   \n2                                -0.023623                         -0.023623   \n3                                -0.023623                         -0.023623   \n4                                -0.023623                         -0.023623   \n...                                    ...                               ...   \n1788                             -0.023623                         -0.023623   \n1789                             -0.023623                         -0.023623   \n1790                             -0.023623                         -0.023623   \n1791                             -0.023623                         -0.023623   \n1792                             -0.023623                         -0.023623   \n\n      BroadBeanOrigin_Venezuela, Java  BroadBeanOrigin_Venezuela, Trinidad  \\\n0                           -0.023623                            -0.023623   \n1                           -0.023623                            -0.023623   \n2                           -0.023623                            -0.023623   \n3                           -0.023623                            -0.023623   \n4                           -0.023623                            -0.023623   \n...                               ...                                  ...   \n1788                        -0.023623                            -0.023623   \n1789                        -0.023623                            -0.023623   \n1790                        -0.023623                            -0.023623   \n1791                        -0.023623                            -0.023623   \n1792                        -0.023623                            -0.023623   \n\n      BroadBeanOrigin_Venezuela/ Ghana  BroadBeanOrigin_Vietnam  \\\n0                            -0.023623                -0.147148   \n1                            -0.023623                -0.147148   \n2                            -0.023623                -0.147148   \n3                            -0.023623                -0.147148   \n4                            -0.023623                -0.147148   \n...                                ...                      ...   \n1788                         -0.023623                -0.147148   \n1789                         -0.023623                -0.147148   \n1790                         -0.023623                -0.147148   \n1791                         -0.023623                -0.147148   \n1792                         -0.023623                -0.147148   \n\n      BroadBeanOrigin_West Africa  BroadBeanOrigin_   \n0                       -0.057945          -0.206014  \n1                       -0.057945          -0.206014  \n2                       -0.057945          -0.206014  \n3                       -0.057945          -0.206014  \n4                       -0.057945          -0.206014  \n...                           ...                ...  \n1788                    -0.057945          -0.206014  \n1789                    -0.057945          -0.206014  \n1790                    -0.057945          -0.206014  \n1791                    -0.057945          -0.206014  \n1792                    -0.057945          -0.206014  \n\n[1793 rows x 1652 columns]\n"
     ]
    }
   ],
   "source": [
    "finalized_feature_matrix.columns = new_data.columns\n",
    "print(finalized_feature_matrix)"
   ]
  },
  {
   "source": [
    "Now let's take a look at our vairence and make sure that is actually makes sense."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Rating                                  0.228166\n",
      "ReviewDate                              8.577083\n",
      "CocoaPercentage                        40.024787\n",
      "CompanyMaker_AMMA                       0.002782\n",
      "CompanyMaker_Acalli                     0.001115\n",
      "                                         ...    \n",
      "BroadBeanOrigin_Venezuela, Trinidad     0.000558\n",
      "BroadBeanOrigin_Venezuela/ Ghana        0.000558\n",
      "BroadBeanOrigin_Vietnam                 0.020756\n",
      "BroadBeanOrigin_West Africa             0.003337\n",
      "BroadBeanOrigin_                        0.039078\n",
      "Length: 1652, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(new_data.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mean:\n\nRating                                 2.018897e-16\nReviewDate                            -2.683328e-14\nCocoaPercentage                       -1.313939e-16\nCompanyMaker_AMMA                     -1.286130e-15\nCompanyMaker_Acalli                    5.108311e-16\n                                           ...     \nBroadBeanOrigin_Venezuela, Trinidad   -8.072414e-17\nBroadBeanOrigin_Venezuela/ Ghana      -8.072801e-17\nBroadBeanOrigin_Vietnam               -3.973088e-16\nBroadBeanOrigin_West Africa           -4.139342e-17\nBroadBeanOrigin_                      -1.039944e-16\nLength: 1652, dtype: float64\n\n"
     ]
    }
   ],
   "source": [
    "# This is the part that we really care about...\n",
    "print(f\"Mean:\\n\\n{finalized_feature_matrix.mean()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Standard Deviation:\n\nRating                                 1.000279\nReviewDate                             1.000279\nCocoaPercentage                        1.000279\nCompanyMaker_AMMA                      1.000279\nCompanyMaker_Acalli                    1.000279\n                                         ...   \nBroadBeanOrigin_Venezuela, Trinidad    1.000279\nBroadBeanOrigin_Venezuela/ Ghana       1.000279\nBroadBeanOrigin_Vietnam                1.000279\nBroadBeanOrigin_West Africa            1.000279\nBroadBeanOrigin_                       1.000279\nLength: 1652, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Standard Deviation:\\n\\n{finalized_feature_matrix.std()}\")"
   ]
  },
  {
   "source": [
    "Excellent,out standard dev and mean look perfect, time to run PCA. PCA will tell us the weight of how important each individual feature is. We will receive a pca level of varience for each feature in order of most important to least imporant."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We will remove the \"Rating\" field from our feature matrix, as that is our target value and we do not want that being apart of our pca analysis. We will store our target vector off to the side while we run PCA. And create our X meature matrix officially."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     Rating\n0  1.181356\n1 -0.912734\n2 -0.389211\n3  0.657834\n4  0.657834\n"
     ]
    }
   ],
   "source": [
    "# Recall that target vector is y.\n",
    "y = finalized_feature_matrix[['Rating']]\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   ReviewDate  CocoaPercentage  CompanyMaker_AMMA  CompanyMaker_Acalli  \\\n0    1.254754        -1.375407          -0.052881            -0.033417   \n1    0.913207        -0.268644          -0.052881            -0.033417   \n2    0.913207        -0.268644          -0.052881            -0.033417   \n3    0.913207        -0.268644          -0.052881            -0.033417   \n4    0.913207        -0.268644          -0.052881            -0.033417   \n\n   CompanyMaker_Adi  CompanyMaker_Aequare (Gianduja)  CompanyMaker_Ah Cacao  \\\n0         -0.047285                        -0.033417              -0.023623   \n1         -0.047285                        -0.033417              -0.023623   \n2         -0.047285                        -0.033417              -0.023623   \n3         -0.047285                        -0.033417              -0.023623   \n4         -0.047285                        -0.033417              -0.023623   \n\n   CompanyMaker_Akesson's (Pralus)  CompanyMaker_Alain Ducasse  \\\n0                        -0.040939                   -0.052881   \n1                        -0.040939                   -0.052881   \n2                        -0.040939                   -0.052881   \n3                        -0.040939                   -0.052881   \n4                        -0.040939                   -0.052881   \n\n   CompanyMaker_Alexandre  ...  BroadBeanOrigin_Venezuela  \\\n0               -0.047285  ...                  -0.368142   \n1               -0.047285  ...                  -0.368142   \n2               -0.047285  ...                  -0.368142   \n3               -0.047285  ...                  -0.368142   \n4               -0.047285  ...                  -0.368142   \n\n   BroadBeanOrigin_Venezuela, Carribean  BroadBeanOrigin_Venezuela, Dom. Rep.  \\\n0                             -0.023623                             -0.023623   \n1                             -0.023623                             -0.023623   \n2                             -0.023623                             -0.023623   \n3                             -0.023623                             -0.023623   \n4                             -0.023623                             -0.023623   \n\n   BroadBeanOrigin_Venezuela, Ghana  BroadBeanOrigin_Venezuela, Java  \\\n0                         -0.023623                        -0.023623   \n1                         -0.023623                        -0.023623   \n2                         -0.023623                        -0.023623   \n3                         -0.023623                        -0.023623   \n4                         -0.023623                        -0.023623   \n\n   BroadBeanOrigin_Venezuela, Trinidad  BroadBeanOrigin_Venezuela/ Ghana  \\\n0                            -0.023623                         -0.023623   \n1                            -0.023623                         -0.023623   \n2                            -0.023623                         -0.023623   \n3                            -0.023623                         -0.023623   \n4                            -0.023623                         -0.023623   \n\n   BroadBeanOrigin_Vietnam  BroadBeanOrigin_West Africa  BroadBeanOrigin_   \n0                -0.147148                    -0.057945          -0.206014  \n1                -0.147148                    -0.057945          -0.206014  \n2                -0.147148                    -0.057945          -0.206014  \n3                -0.147148                    -0.057945          -0.206014  \n4                -0.147148                    -0.057945          -0.206014  \n\n[5 rows x 1651 columns]\n"
     ]
    }
   ],
   "source": [
    "X = finalized_feature_matrix.drop(['Rating'], axis=1)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2.57840525e-03 2.47942508e-03 2.43544133e-03 ... 4.24498629e-35\n 3.92819187e-35 3.74440466e-35]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "print(pca.explained_variance_ratio_)\n",
    "# print(pca.singular_values_)"
   ]
  },
  {
   "source": [
    "## Model 1\n",
    "\n",
    "I am going to attempt to train this data using a 75/25 train/test split with Linear Regression.\n",
    "\n",
    "However, based on the number of features we have and the small number of columns that we have, I expect this model to overfit a lot and give as really poor score. The number of features is actually extremely close to the number of rows that we have in our feature matrix, which is a bad sign right off the bat."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train size: 1344\nX_test size: 449\n\ny_train size: 1344\ny_test size: 449\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.25)\n",
    "\n",
    "print(f\"X_train size: {len(X_train)}\")\n",
    "print(f\"X_test size: {len(X_test)}\\n\")\n",
    "\n",
    "print(f\"y_train size: {len(y_train)}\")\n",
    "print(f\"y_test size: {len(y_test)}\")\n"
   ]
  },
  {
   "source": [
    "### Linear Regression on training data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear Regression Training Score: 39.137388531437104 percent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg = reg.fit(X_train, y_train)\n",
    "print(f\"Linear Regression Training Score: {reg.score(X_train, y_train) * 100} percent\")"
   ]
  },
  {
   "source": [
    "### Linear Regression on testing data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Linear Regression Testing Score: -5.264922401675303e+29 percent\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg.fit(X_test, y_test)\n",
    "print(f\"Linear Regression Testing Score: {reg.score(X_train, y_train) * 100} percent\")"
   ]
  },
  {
   "source": [
    "## Model 2\n",
    "\n",
    "As predicted, the linear regression model preformed horribly.\n",
    "\n",
    "Here is what we are going to do:\n",
    "\n",
    "1. Use cross validation for training/testing our model, as we do not have a lot of data and this will help to use everything that we have got  for training and for testing.\n",
    "\n",
    "2. We are going try out using Lasso Regression (regularization) because it appears that we are STRONGLY overfitting and it has to do with the fact that we have a huge amount of features. Lasso is actually really good at zeroing out features that are not of use to us. In this case that zeroing out of features will strongly increase our performance, so we will use this instead of Ridge.\n",
    "\n",
    "3. In order to find the best possible learning rate for our Lasso model, we will run GridSearch with a bunch of different possible learning rates. This will give us the best model with the best learning rate and show us the best score for that combonation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "I am going to skip ahead and display the best model and tunning paramaters right now simply because GridSearch takes a while to run.\n",
    "The code for GridSearch and how I found the best model is at the bottom of the notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lasso Score: 53.87798809200481\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV, Lasso\n",
    "from numpy import absolute, arange\n",
    "\n",
    "model = LassoCV(alphas=[0.02], cv=5, random_state=0).fit(X, y)\n",
    "print(f'Lasso Score: {absolute(model.score(X,  y) * 100)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
    "from numpy import absolute, arange\n",
    "\n",
    "# Lasso Approach.\n",
    "# Note I am using lasso over ridge because I have a LOT of features and I actually want many of them to be zeroed out.\n",
    "# In addition to this, we will be running Cross Validation because we do not have a lot of training data to offer this model.\n",
    "# lasso = Lasso()\n",
    "# cv = RepeatedKFold(n_splits=10)\n",
    "# grid = {'alpha': arange(0, 1, 0.01)}\n",
    "# search = GridSearchCV(lasso, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# results = search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-ef2c34974a53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Best score: {absolute(results.best_score_) * 100}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Params: {results.best_params_}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'Best score: {absolute(results.best_score_) * 100}')\n",
    "print(f'Params: {results.best_params_}')"
   ]
  },
  {
   "source": [
    "### Things to do\n",
    "\n",
    "1. Look more into the results we got from PCA and see if we can cut a bunch of features off. Because if we can, then linear regression or ridge might be useful.\n",
    "\n",
    "2. Look into GridSearchCV v.s. LassoCV"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Model 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's take a moment to train and run our data on a Neural Network and see what our results look like. Neural Networks (in general) are better at fitting non-linear data. In other words, we can use a NN when running with a lot of features.\n",
    "\n",
    "This will actually be a good test here because we can run the NN with a logistic regression based activation function (Relu) and see if we are overfitting because we do not have enough data, or we are overfitting because there is too many features.\n",
    "\n",
    "If we are overfitting because of too little data, then in theory, our NN should also have issues getting a good overall score on our data.\n",
    "\n",
    "If we are not overfitting, then the number of training records we are feeding out network should be fine."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Network Structure\n",
    "\n",
    "First let's create our the structure for our Neural Network.\n",
    "\n",
    "Since this data does not seem all that complex (to my knowledge), we will start with a simple structure. One hidden layer.\n",
    "\n",
    "Since we are simply making a single prediction, we will use one output node in our layer.\n",
    "\n",
    "We will use m nodes in our hidden layer, where m = (number of rows of data we have in our data frame).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports needed.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.metrics import MeanAbsolutePercentageError\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function that creates and returns our NN structure.\n",
    "def build_model():\n",
    "    # Train, test approach. (This should work worse than with cross validation, but I want to test it).\n",
    "    # We will make the input layer node count = number of features we have.\n",
    "    # We will make each layer have m number of nodes, where m is the number of rows in our dataframe.\n",
    "    # We will only have one output, since we are doing Regression prediction.\n",
    "    model = Sequential()\n",
    "    model.add(Dense(X.shape[0], input_dim=X.shape[1], activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=[MeanAbsolutePercentageError(), 'mse'])\n",
    "    return model\n",
    "\n",
    "    # Our NN structure.\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use mean squared error as our loss function, as we are basically running linear regression.\n",
    "# We will use the optimizer function \"adam\". This is rather popular and is a good optimized verison of standard gradient descent.\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=[MeanAbsolutePercentageError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.3516 - mean_absolute_percentage_error: 113.1688\n",
      "Epoch 2/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.1222 - mean_absolute_percentage_error: 100.6863\n",
      "Epoch 3/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 4/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 5/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 6/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 7/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 8/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 9/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 10/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 11/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 12/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 13/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 14/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 15/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 16/150\n",
      "135/135 [==============================] - 1s 9ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 17/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 18/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 19/150\n",
      "135/135 [==============================] - 1s 9ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 20/150\n",
      "135/135 [==============================] - 1s 9ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 21/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 22/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 23/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 24/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 25/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 26/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 27/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 28/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 29/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 30/150\n",
      "135/135 [==============================] - 1s 9ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 31/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 32/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 33/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 34/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 35/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 36/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 37/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 38/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 39/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 40/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 41/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 42/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 43/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 44/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 45/150\n",
      "135/135 [==============================] - 1s 9ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 46/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 47/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 48/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 49/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 50/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 51/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 52/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 53/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 54/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 55/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 56/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 57/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 58/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 59/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 60/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 61/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 62/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 63/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 64/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 65/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 66/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 67/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 68/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 69/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 70/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 71/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 72/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 73/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 74/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 75/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 76/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 77/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 78/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 79/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 80/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 81/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 82/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 83/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 84/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 85/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 86/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 87/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 88/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 89/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 90/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 91/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 92/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 93/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 94/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 95/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 96/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 97/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 98/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 99/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 100/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 101/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 102/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 103/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 104/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 105/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 106/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 107/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 108/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 109/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 110/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 111/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 112/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 113/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 114/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 115/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 116/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 117/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 118/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 119/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 120/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 121/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 122/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 123/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 124/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 125/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 126/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 127/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 128/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 129/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 130/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 131/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 132/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 133/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 134/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 135/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 136/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 137/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 138/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 139/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 140/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 141/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 142/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 143/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 144/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 145/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 146/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 147/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 148/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 149/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n",
      "Epoch 150/150\n",
      "135/135 [==============================] - 1s 10ms/step - loss: 1.0147 - mean_absolute_percentage_error: 100.0000\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19136c1de80>"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "# Train model on the training data...\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "42/42 [==============================] - 0s 3ms/step - loss: 1.0147 - mean_absolute_error: 0.8106\n",
      "Loss: 1.0147128105163574\n",
      "Accuracy Percentage: 81.060129404068\n"
     ]
    }
   ],
   "source": [
    "# Let's see our accuracy on the training data...\n",
    "loss, accuracy = model.evaluate(X_train, y_train)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy Percentage: {accuracy * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0908 - mean_absolute_error: 0.7976\n",
      "Loss: 1.0907658338546753\n",
      "Accuracy Percentage: 79.75571751594543\n"
     ]
    }
   ],
   "source": [
    "# Let's see how well we do on testing data...\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy Percentage: {accuracy * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Results\n",
    "\n",
    "It appears that our training accuracy is 81 percent and our testing accuracy is 79 percent. That is not at all bad. When we compare these scores to Linear regression or Lasso, they knock it right out of the park.\n",
    "\n",
    "This really shows us that a Neural Network can handle lots tons of features much better than that of Linear Regression or even tweaked Lasso Regression. We also get the added benefit of not even worrying about feature reduction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Model 4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We are going to use the exact same Neural Netowork, but this time instead of using a 80/20 split, we are going to use cross validation. This should improve our NN's scores even more."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I am here\n"
     ]
    }
   ],
   "source": [
    "# We will use the exact same number of folds as we did with Lasso.\n",
    "estimator = KerasRegressor(build_fn=build_model, epochs=150, batch_size=10)\n",
    "k_fold = RepeatedKFold(n_splits=10)\n",
    "print(\"I am here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_val_score(estimator, X, y, cv=k_fold, n_jobs=-1)\n",
    "print(f\"Results: {results}\")\n",
    "results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}